{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from matplotlib.patheffects import withStroke\n",
    "import pandas as pd\n",
    "import os \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../dataset_nyc_taxi_samples/parquet/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_dict(names: List[str], csv_root: str) -> Dict[str, pd.DataFrame]:\n",
    "    csv_dict = {}\n",
    "\n",
    "    for name in names:\n",
    "        csv_dict[name] = {}\n",
    "        csv_dict[name][\"train\"] = pd.read_csv(os.path.join(csv_root, f\"{name}_train_1M.csv\"))\n",
    "\n",
    "        csv_dict[name][\"valid\"] = pd.read_csv(os.path.join(csv_root, f\"{name}_valid_500k.csv\"))\n",
    "    \n",
    "    return csv_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4304/649362036.py:6: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_dict[name][\"train\"] = pd.read_csv(os.path.join(csv_root, f\"{name}_train_1M.csv\"))\n",
      "/tmp/ipykernel_4304/649362036.py:8: DtypeWarning: Columns (23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_dict[name][\"valid\"] = pd.read_csv(os.path.join(csv_root, f\"{name}_valid_500k.csv\"))\n"
     ]
    }
   ],
   "source": [
    "names = [\"fhv\", \"green\", \"yellow\"]\n",
    "\n",
    "csv_dict = get_csv_dict(names, \"../dataset_nyc_taxi_samples/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_columns(df: pd.DataFrame, taxi_type: str) -> pd.DataFrame:\n",
    "\n",
    "    if taxi_type == \"green\":\n",
    "        df['lpep_pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "        df['lpep_dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "\n",
    "        df['store_and_fwd_flag'] = df['store_and_fwd_flag'] == 'Y'\n",
    "        df['store_and_fwd_flag'] = df['store_and_fwd_flag'].astype(int)\n",
    "\n",
    "        df['trip_time'] = df['lpep_dropoff_datetime'] - df['lpep_pickup_datetime']\n",
    "        df['trip_time'] = df['trip_time'].values.astype(float)//10**9\n",
    "\n",
    "        df['month'] = df['lpep_pickup_datetime'].dt.month\n",
    "        df['weekday'] = df['lpep_pickup_datetime'].dt.weekday\n",
    "        df['hour'] = df['lpep_pickup_datetime'].dt.hour\n",
    "\n",
    "        df = df.drop(columns=['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'ehail_fee', 'total_amount'])\n",
    "\n",
    "\n",
    "    if taxi_type == \"yellow\":\n",
    "        df['tpep_pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "        df['tpep_dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "\n",
    "        df['store_and_fwd_flag'] = df['store_and_fwd_flag'] == 'Y'\n",
    "        df['store_and_fwd_flag'] = df['store_and_fwd_flag'].astype(int)\n",
    "\n",
    "        df['trip_time'] = df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']\n",
    "        df['trip_time'] = df['trip_time'].values.astype(float)//10**9\n",
    "\n",
    "        df['month'] = df['tpep_pickup_datetime'].dt.month\n",
    "        df['weekday'] = df['tpep_pickup_datetime'].dt.weekday\n",
    "        df['hour'] = df['tpep_pickup_datetime'].dt.hour\n",
    "\n",
    "        df = df.drop(columns=['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'airport_fee', 'total_amount'])\n",
    "\n",
    "    if taxi_type == \"fhv\":\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])\n",
    "        df['request_datetime'] = pd.to_datetime(df['request_datetime'])\n",
    "\n",
    "        for flag in ['shared_request_flag', 'shared_match_flag', 'access_a_ride_flag', 'wav_request_flag', 'wav_match_flag']:\n",
    "            df[flag] = df[flag] == 'Y'\n",
    "            df[flag] = df[flag].astype(int)\n",
    "\n",
    "        df['trip_time'] = df['dropoff_datetime'] - df['pickup_datetime']\n",
    "        df['trip_time'] = df['trip_time'].values.astype(float)//10**9\n",
    "\n",
    "        df['wait_time'] = df['pickup_datetime'] - df['request_datetime']\n",
    "        df['wait_time'] = df['wait_time'].values.astype(float)//10**9\n",
    "\n",
    "        df['month'] = df['pickup_datetime'].dt.month\n",
    "        df['weekday'] = df['pickup_datetime'].dt.weekday\n",
    "        df['hour'] = df['pickup_datetime'].dt.hour\n",
    "\n",
    "        df = df.drop(columns=['hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'airport_fee',\n",
    "                                        'request_datetime', 'on_scene_datetime', 'pickup_datetime', 'dropoff_datetime',])\n",
    "    \n",
    "    df = df.dropna()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile = gpd.read_file(\"../taxi_zones\")\n",
    "location_to_borough_mapping = dict(zip(shapefile['LocationID'], shapefile['borough']))\n",
    "borough_to_borough_idx_mappng = {'Manhattan': 0, 'Queens': 1, 'Brooklyn': 2, 'Bronx': 3, 'EWR': 4, 'Staten Island': 5}\n",
    "\n",
    "location_to_borough_idx_mapping = dict(zip(shapefile['LocationID'], shapefile['borough'].map(borough_to_borough_idx_mappng)))\n",
    "shapefile['borough_id'] = shapefile['borough'].map(borough_to_borough_idx_mappng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fhv - train reduced: 0.04675354286691461%, num samples: 46752\n",
      "fhv - valid reduced: 0.046842594722158334%, num samples: 23422\n",
      "green - train reduced: 0.009437283118493555%, num samples: 9437\n",
      "green - valid reduced: 0.00951161953521859%, num samples: 4756\n",
      "yellow - train reduced: 0.005849181324621063%, num samples: 5849\n",
      "yellow - valid reduced: 0.0057577927194620996%, num samples: 2879\n"
     ]
    }
   ],
   "source": [
    "for name in names:\n",
    "    # change id, map borough to id\n",
    "    csv_dict[name][\"train\"][\"PUBoroughID\"] = (csv_dict[name][\"train\"][\"PULocationID\"]-1).map(location_to_borough_idx_mapping)\n",
    "    csv_dict[name][\"train\"][\"DOBoroughID\"] = (csv_dict[name][\"train\"][\"DOLocationID\"]-1).map(location_to_borough_idx_mapping)\n",
    "    csv_dict[name][\"valid\"][\"PUBoroughID\"] = (csv_dict[name][\"valid\"][\"PULocationID\"]-1).map(location_to_borough_idx_mapping)\n",
    "    csv_dict[name][\"valid\"][\"DOBoroughID\"] = (csv_dict[name][\"valid\"][\"DOLocationID\"]-1).map(location_to_borough_idx_mapping)\n",
    "\n",
    "    # drop nans on PUBoroughID, DOBoroughID\n",
    "    len_train_before = len(csv_dict[name][\"train\"])\n",
    "    len_valid_before = len(csv_dict[name][\"valid\"])\n",
    "\n",
    "    csv_dict[name][\"train\"].dropna(subset=[\"PUBoroughID\", \"DOBoroughID\"], inplace=True)\n",
    "    csv_dict[name][\"valid\"].dropna(subset=[\"PUBoroughID\", \"DOBoroughID\"], inplace=True)\n",
    "\n",
    "    csv_dict[name][\"train\"] = update_columns(csv_dict[name][\"train\"], name)\n",
    "    csv_dict[name][\"valid\"] = update_columns(csv_dict[name][\"valid\"], name)\n",
    "\n",
    "    len_train_after = len(csv_dict[name][\"train\"])\n",
    "    len_valid_after = len(csv_dict[name][\"valid\"])\n",
    "\n",
    "    print(f\"{name} - train reduced: {(len_train_before-len_train_after)/len_train_before}%, num samples: {len_train_before-len_train_after}\")\n",
    "    print(f\"{name} - valid reduced: {(len_valid_before-len_valid_after)/len_valid_before}%, num samples: {len_valid_before-len_valid_after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['VendorID', 'passenger_count', 'trip_distance', 'RatecodeID',\n",
       "       'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type',\n",
       "       'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount',\n",
       "       'improvement_surcharge', 'congestion_surcharge', 'retail_price',\n",
       "       'temperature_2m (°C)', 'relative_humidity_2m (%)', 'dew_point_2m (°C)',\n",
       "       'apparent_temperature (°C)', 'precipitation (mm)', 'rain (mm)',\n",
       "       'snowfall (cm)', 'snow_depth (m)', 'surface_pressure (hPa)',\n",
       "       'cloud_cover (%)', 'wind_speed_10m (km/h)', 'is_day ()', 'PUBoroughID',\n",
       "       'DOBoroughID', 'trip_time', 'month', 'weekday', 'hour'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict[name][\"train\"].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "fhv\n",
      "Mean Squared Error: 4.634459390855498\n",
      "Mean Absolute Error: 1.2076065665555729\n",
      "Coefficients: [ 1.14763642e-02  3.04155523e-03 -7.50486512e+00 -1.08085120e+01\n",
      "  1.61764647e+01  2.54430433e+00  1.45238411e+01  7.06464344e+00\n",
      "  6.25838932e-01  1.10321964e+01  1.29037965e-01  1.84089379e-01\n",
      " -7.71150991e-02  8.17171175e-02 -7.36178179e-02  1.33669326e-01\n",
      " -8.62463779e-01  1.14822706e-02 -5.05785535e-01  1.33511294e+00\n",
      " -5.54391149e+01  5.54966662e+01  7.21090827e+00 -1.42565339e-01\n",
      " -3.13153731e-02  3.22218704e-02  1.04549289e-01  1.19423358e-01\n",
      " -1.07065984e-01 -1.14407463e-01 -3.27681050e-01  8.09519432e-02\n",
      " -7.37092096e-02  7.49998501e-02]\n",
      "Intercept: -0.417857295368977\n",
      "====================\n",
      "====================\n",
      "green\n",
      "Mean Squared Error: 6.683244293988039\n",
      "Mean Absolute Error: 1.3983482675412022\n",
      "Coefficients: [ 5.78810666e-01  1.20580004e-01  1.16809719e+01  1.06099068e-01\n",
      " -9.70010736e-02  6.29694650e-01 -2.68496017e-01  3.81232313e+01\n",
      "  4.07895032e+00 -6.23860520e+00  1.70673167e+00  2.53097891e+00\n",
      "  1.34374734e-11 -1.13911379e+00  1.62185349e+00  6.89215535e-01\n",
      "  1.43624376e+00  3.15790360e-01 -5.49212297e-01 -7.60548093e-01\n",
      " -1.10951239e+02  1.11201076e+02  1.42683123e+01 -1.37323298e-01\n",
      " -9.18112168e-02 -2.11388133e-02 -8.36711646e-02 -2.45473110e-01\n",
      "  2.42088918e-01 -3.71456009e-02 -2.20550179e+00  2.91957978e-01\n",
      "  2.82086467e-01  1.09677222e-01]\n",
      "Intercept: 1.3367927818067282\n",
      "====================\n",
      "====================\n",
      "yellow\n",
      "Mean Squared Error: 3.9543684977364024\n",
      "Mean Absolute Error: 0.9597584872731378\n",
      "Coefficients: [ 1.83834499e+00  6.37835560e-02  2.71110058e+01 -5.34782384e+00\n",
      " -3.15069406e-02 -3.07038263e-02 -3.34594885e-03 -5.15143483e-13\n",
      "  7.65295984e+01  3.15999713e+00  6.66946651e+00  2.29231436e+01\n",
      "  1.44360948e-01  1.62292985e+00  1.76691749e-01 -3.45206913e-01\n",
      " -2.15407936e-02 -2.60712993e-01  5.05291251e-01 -7.65316862e+00\n",
      "  7.68057833e+00  1.13998473e+00 -1.97679439e-02  4.05595257e-03\n",
      "  3.33952837e-02  6.51298903e-02  4.13991793e-02 -7.88075544e-02\n",
      " -4.31901262e-02 -1.71998864e+00  7.27008900e-02  3.59844995e-03\n",
      "  5.01334933e-02]\n",
      "Intercept: -7.237804507111543\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "for name in names:\n",
    "    scaler = MinMaxScaler()\n",
    "    tip_label = [label for label in csv_dict[name][\"train\"].columns if \"tip\" in label][0]\n",
    "\n",
    "    y_train = csv_dict[name][\"train\"].copy()[tip_label].to_numpy()\n",
    "    y_val = csv_dict[name][\"valid\"].copy()[tip_label].to_numpy()\n",
    "\n",
    "    x_train = csv_dict[name][\"train\"].copy().drop(columns=[tip_label]).to_numpy()\n",
    "    x_val = csv_dict[name][\"valid\"].copy().drop(columns=[tip_label]).to_numpy()\n",
    "\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_val = scaler.transform(x_val)\n",
    "\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    print(\"====================\")\n",
    "    print(name)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "    print(\"====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tip_amount'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tip_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         18.55\n",
       "1          2.96\n",
       "2          1.66\n",
       "3          2.76\n",
       "4          1.76\n",
       "          ...  \n",
       "500013     4.45\n",
       "500014     3.28\n",
       "500015     3.35\n",
       "500016     6.52\n",
       "500017     4.54\n",
       "Name: tip_amount, Length: 497139, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dict[name][\"valid\"][tip_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994120, 34)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994120, 34)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.fit_transform(x_train, y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "\n",
    "msno.bar(train_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab_studia",
   "language": "python",
   "name": "lab_studia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
